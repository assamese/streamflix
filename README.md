# Project: Data Pipeline for Content Completion Prediction

## Overview (Phase I, Deliverable I)

This project implements a data pipeline designed to predict the likelihood of a user completing a specific piece of content. The pipeline processes raw event data, enriches it with user and content metadata, and generates a set of features used to train a machine learning model. The final output is a table (`completion_prediction_features`) that can be used to train a model.

## Data Flow

The data flow can be summarized as follows:

1.  **Raw Data Ingestion**: Raw event data is ingested into the `events_table`.

2.  **Data Cleaning**: The `clean_events.py` script processes the raw event data in the `events_table` and generates the `cleaned_events` table.

3.  **Data Enrichment and Feature Engineering**: The `create_features.py` script performs several operations:

    * It joins the `cleaned_events` table with user metadata from the `users_dim` table and content metadata from the `content_dim` table.

    * It calculates various intermediate tables (`session_starts`, `session_identification`, `content_completion`, etc.) to derive relevant features.

    * It generates the final `completion_prediction_features` table, which contains the features required for model training.

4.  **Model Training**: The `completion_prediction_features` table is then used to train a machine learning model (e.g., Logistic Regression) to predict content completion.

## Table Descriptions

### Input Tables

* `events_table`: Contains raw event data, such as user interactions with content (e.g., play, pause, stop, complete).

* `users_dim`: Contains user metadata, such as user ID, signup date, and plan tier.

* `content_dim`: Contains content metadata, such as content ID, genre, runtime, release year, and maturity rating.

### Intermediate Tables

These tables are generated by the `create_features.py` script and are used in the feature engineering process:

* `cleaned_events`: Contains cleaned event data, generated from `events_table` by `clean_events.py`.

* `enriched_table`: Contains event data enriched with user and content metadata.

* `session_starts`: Contains information about the start of user sessions.

* `session_identification`: Identifies user sessions based on event timestamps.

* `content_completion`: Contains information about content completion status for each session.

* `user_activity`: Contains aggregated user activity information.

* `user_engagement`: Contains aggregated user engagement information.

* `user_genre_preference`: Contains information about user genre preferences.

* `user_completion_rate`: Contains information about user completion rates.

* `content_completion_rate`: Contains information about content completion rates.

* `avg_view_duration_ratio`: Contains the average view duration ratio.

* `has_played_before`: Indicates if a user has played a content before.

* `number_of_pauses`: Contains the number of pauses during a content play.

### Final Table

* `completion_prediction_features`: Contains the final set of features used for training the content completion prediction model. This table is generated by the `create_features.py` script and depends on many of the intermediate tables.

## Script Descriptions

* `clean_events.py`: This script is responsible for cleaning and preprocessing the raw event data from the `events_table`. It generates the `cleaned_events` table.

* `create_features.py`: This script is the core of the feature engineering process. It joins data from various tables, calculates relevant features, and generates the final `completion_prediction_features` table.

## Dependencies

### Software Dependencies

* Python 3.x

* PostgreSQL

* psycopg2 (PostgreSQL database adapter for Python)

* pandas

* scikit-learn

* matplotlib

### Python Libraries

The following Python libraries are used in this project:

* `pandas`: For data manipulation and analysis.

* `psycopg2`: For connecting to the PostgreSQL database.

* `sklearn`: For machine learning (e.g., Logistic Regression), data preprocessing, and model evaluation.

* `matplotlib`: For plotting and data visualization.

* `subprocess`: To install missing dependencies.

* `io`: For handling input/output operations.

* `warnings`: For suppressing warnings.

## Setup Instructions

1.  **Install Dependencies**: Use `pip` to install the required Python packages:

    ```
    pip install pandas psycopg2 scikit-learn matplotlib
    ```

    The provided code includes a function to install missing dependencies.

2.  **Set up PostgreSQL Database**:

    * Create a PostgreSQL database.

    * Ensure that the input tables (`events_table`, `users_dim`, `content_dim`) are populated with the necessary data.

3.  **Configure Database Connection**:

    * Set the database connection details (host, port, name, user, password) in the `main()` function of the provided Python script or via environment variables. The password should be set in the `POSTGRES_PASSWORD` environment variable.

4.  **Run the Scripts**:

    * Run the `clean_events.py` script to generate the `cleaned_events` table.

    * Run the `create_features.py` script to generate the `completion_prediction_features` table.

5.  **Train the Model**: Use the `completion_prediction_features` table to train your machine learning model. The provided code includes an example of training a Logistic Regression model.

## Important Considerations

* **Data Quality**: The accuracy of the completion prediction heavily relies on the quality and completeness of the input data. Ensure that the input tables are accurate and up-to-date.

* **Feature Engineering**: The features in the `completion_prediction_features` table are crucial for model performance. Carefully consider which features to include and how to engineer them.

* **Model Selection and Evaluation**: Choose an appropriate machine learning model for the prediction task, and evaluate its performance using relevant metrics (e.g., precision, recall, F1-score, AUC-ROC).

* **Scalability**: For large datasets, consider optimizing the SQL queries and the data processing steps for better performance. You might also want to use a distributed computing framework.

* **Error Handling**: The provided code includes error handling for database connections and missing dependencies. Ensure that you implement robust error handling in your production environment.

* **Class Imbalance**: The dataset might have a class imbalance, where there are more non-completions than completions. Consider using appropriate techniques to handle this imbalance, such as oversampling, undersampling, or class weighting.
